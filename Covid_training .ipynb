{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created Dataset\n",
    "\n",
    "#these are all image files \n",
    "\n",
    "TRAIN_PATH = \"CovidDataset/Train\"\n",
    "VAL_PATH = \"CovidDataset/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import * \n",
    "from keras.preprocessing import image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Based Model in Keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(224,224,3)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 220, 220, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 108, 108, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                5537856   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,668,097\n",
      "Trainable params: 5,668,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from scratch\n",
    "train_datagen = image.ImageDataGenerator(  #from keras \n",
    "    rescale = 1./255,   #help in normalisation\n",
    "    \n",
    "    #to take random crops from images \n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,  #no vertical flip because we should not invert the xray\n",
    ")\n",
    "\n",
    "test_dataset = image.ImageDataGenerator(rescale=1./255)  #only rescaling , no augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 224 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(   #train_generator is an obj which has am method flow_from_directory\n",
    "    'CovidDataset/Train',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid': 0, 'Normal': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices   #2 classes which are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_dataset.flow_from_directory(\n",
    "    'CovidDataset/Val',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 250s 31s/step - loss: 0.9025 - accuracy: 0.5938 - val_loss: 0.6771 - val_accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 131s 16s/step - loss: 0.6083 - accuracy: 0.6719 - val_loss: 0.5567 - val_accuracy: 0.8167\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 112s 14s/step - loss: 0.4845 - accuracy: 0.7695 - val_loss: 0.4011 - val_accuracy: 0.8833\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 114s 14s/step - loss: 0.2891 - accuracy: 0.8984 - val_loss: 0.1231 - val_accuracy: 0.9667\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 111s 14s/step - loss: 0.2158 - accuracy: 0.9336 - val_loss: 0.1043 - val_accuracy: 0.9500\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 105s 13s/step - loss: 0.2323 - accuracy: 0.9297 - val_loss: 0.1136 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 106s 13s/step - loss: 0.2021 - accuracy: 0.9258 - val_loss: 0.0826 - val_accuracy: 0.9667\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 104s 13s/step - loss: 0.1923 - accuracy: 0.9336 - val_loss: 0.0677 - val_accuracy: 0.9667\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 106s 13s/step - loss: 0.1739 - accuracy: 0.9336 - val_loss: 0.1321 - val_accuracy: 0.9667\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 105s 13s/step - loss: 0.1174 - accuracy: 0.9570 - val_loss: 0.0545 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs = 10,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_adv.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.032667938619852066, 0.9732142686843872]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04450663551688194, 1.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_adv.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Covid': 0, 'Normal': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual= []\n",
    "y_test =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"CovidDataset/Val/Normal/\"):\n",
    "    img = image.load_img(\"CovidDataset/Val/Normal/\"+ i , target_size=(224,224))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    p=model.predict_classes(img)\n",
    "    y_test.append(p[0,0])\n",
    "    y_actual.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"CovidDataset/Val/Covid/\"):\n",
    "    img = image.load_img(\"CovidDataset/Val/Covid/\"+ i , target_size=(224,224))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    p=model.predict_classes(img)\n",
    "    y_test.append(p[0,0])\n",
    "    y_actual.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_actual)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute confusion matrix to evaluate the accuracy of a classification. By definition a confusion matrix is such that C i , j is equal to the number of observations known to be in group and predicted to be in group ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_actual,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22c82737488>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOsUlEQVR4nO3dcYxV5Z3G8eeZwSYGukalEpbFAMKupUkXG9ba2CYQmhVtWtRYg6uVbEmmiUsq1iYQkwYX04Y/tm12G9NmuqBEKQTbmlLT1lXChqi7VdtSwSDVEgRkBBSjJmMDc89v/7i33QkMc+8w973n3JfvJzmZO+fOvPcXmXnm53ve+x5HhAAA6fSUXQAA5I6gBYDECFoASIygBYDECFoASGxC6hd49+QsljXgDNMv6iu7BFTQex+s9njHGEvmXPSh/eN+vVbQ0QJAYsk7WgDoqKK37ArOQNACyIpr1fsfdYIWQFZcdGTadUwIWgBZcVF2BWciaAHkhaAFgLRcwQWlBC2ArDB1AACJuVa9lpagBZAXOloASMsFHS0ApEVHCwBpseoAABLzUNkVnImgBZCXCt5wlqAFkBXW0QJAagQtAKTFxTAASI2OFgDSco39aAEgLTpaAEiMoAWAxLgYBgBpcc8wAEiNi2EAkBhztACQWAXnaHvKLgAA2qpw68cobE+3vcP2Xtsv2767cf5+22/Y3tU4bmhWEh0tgLxE2+ZohyTdGxG/tf1hSb+x/VTjue9GxL+1OhBBCyAr7dq9KyIGJA00Hr9ve6+kaecyFlMHAPJSc8uH7T7bLw47+kYa0vYMSVdJ+nXj1ArbL9neYPviZiURtADyMoY52ojoj4j5w47+04ezPUnSTyStjIj3JH1f0hWS5qne8X67WUlMHQDIS/vmaGX7AtVDdlNE/FSSIuLosOd/KOmJZuPQ0QLISzGGYxS2LWm9pL0R8Z1h56cO+7KbJO1pVhIdLYC8tK+jvVbSlyTttr2rce4+SbfZnqf6it0Dkr7SbCCCFkBWYgx7HYz2lRHxzFm+5BdjrYmgBZAX9joAgMTaeDGsXQhaAHlhm0QASIyOFgASo6MFgLSCi2EAkBhTBwCQGFMHAJAYHS0AJEZHCwBpRQXvGUbQAshLrXqbEhK0ALISzNECQGLM0Z4/jr4p3X9fr95+y3KPdNMthZbeUegP+6R1a3v1waA1dVpo7bqaJk0qu1qU4cEf3KDF11+h48cHdc389WWXk48KdrTVm8zIRG+vdPfXa9q6bUgbNg3psS092v9H6ZtrerViZaHNjw9pwaJCjz7EP8H5atMju3Xzkq1ll5GdCLd8dErT33LbV9peZfs/bP974/FHO1FcN5v8EenKufXHEydKM2eGjh+1Dh6wrppfvyz6yU+FdjxN0J6vnnv2kN458aeyy8hPm25l006j/pbbXiVpi+q7jD8v6YXG4822V6cvLw9H3pD2vWJ97OOhWbNDO3fU/5I+/WSPjr5ZcnFAZqLW0/LRKc1eabmkf4iIdRHxaONYJ+nqxnMjGn6v9If/87121tt1Bgel1fdM0NdW1ediv7G2ph9v6dGdt07Q4KA04YKyKwQyE2796JBmF8MKSX8t6fXTzk/VKI13497o/ZL07slZFVw+3BlDp6RV9/Tqus8VWvjZ+n+GGbOk7/XXJEmvH5Ce3Vm9iXugm3Xj8q6VkrbbflXSoca5yyXNlrQiZWHdLkJ6YE2vZs4K3b7s//8mnXhbuuRSqSikDf29uvnWDk4UAeeDblveFRG/sv23qk8VTFN9fvawpBciotaB+rrW739n/fLnPZo9J3T7LfUZmru+WtOhg9ZjW+qfL1xU6PM3nrcN/3lvw8Yv6NOfuVyXTr5Qe1+7S9964Bk9svGlssvqfl3Y0SoiCkn/24FasjLvE6Hnd58a4ZnQ0jvoYiF9edm2skvIEht/A0Bi3ThHCwDdhaAFgLSi2y6GAUDXoaMFgLSYowWAxFh1AACJVbGjZesoAHkp3PoxCtvTbe+wvdf2y7bvbpy/xPZTtl9tfLy4WUkELYCstHE/2iFJ90bERyVdI+lfbM+VtFrS9oiYI2l74/NREbQA8tKm3bsiYiAiftt4/L6kvapvRbBE0sbGl22UdGOzkghaAFkZS0c7fEvXxtE30pi2Z0i6StKvJU2JiIH6a8WApMua1cTFMABZGcuqg+Fbup6N7UmSfiJpZUS8Z4/9YhtBCyAr7Vx1YPsC1UN2U0T8tHH6qO2pETFge6qkY83GYeoAQF7aNEfreuu6XtLeiPjOsKe2SVrWeLxM0s+alURHCyArbdzr4FpJX5K02/auxrn7JK2TtNX2ckkHJX2x2UAELYCstGvqICKeUf1mByNZNJaxCFoAWYmiejOiBC2ArEQFb2BC0ALISwX3OiBoAWSlipvKELQAskLQAkBqBC0ApFXUWHUAAGlF2QWciaAFkBXmaAEgMYIWABJr414HbUPQAsgKb8EFgMSYOgCAxIJVBwCQFh0tAKTGxTAASIuOFgASK1h1AABp0dECQGoELQCkxa1sACAxpg4AIDGCFgASY9UBAKRGRwsAaTF1AACJEbQAkBjLuwAgMS6GAUBiVZw6qF70A8A4RLjloxnbG2wfs71n2Ln7bb9he1fjuKHZOAQtgKy0M2glPSxp8QjnvxsR8xrHL5oNwtQBgKy0c+ogInbanjHecZIH7fSL+lK/BLrQoXf7yy4BlbR6/EOM4Q4LtvskDQ+p/oho5Ydzhe07Jb0o6d6IeGe0L2bqAEBWiqKn5SMi+iNi/rCjlZD9vqQrJM2TNCDp282+gakDAFlJfRfciDj658e2fyjpiWbfQ9ACyErq5V22p0bEQOPTmyTtGe3rJYIWQGbaGbS2N0taIGmy7cOS1khaYHuepJB0QNJXmo1D0ALISptXHdw2wun1Yx2HoAWQlSq+M4ygBZCVola9xVQELYCs0NECQGIELQAkRtACQGIELQAkxsbfAJBYjGFTmU4haAFkhakDAEgs9aYy54KgBZCVgo4WANJi6gAAEmPVAQAkRkcLAImxvAsAEqOjBYDECFoASIygBYDEaqw6AIC06GgBILEoyq7gTAQtgKzQ0QJAYux1AACJ8RZcAEiMqQMASIypAwBIjI2/ASAxNpUBgMSYowWAxGoV7Girtw4CAMYhwi0fzdjeYPuY7T3Dzl1i+ynbrzY+XtxsHIIWQFaKcMtHCx6WtPi0c6slbY+IOZK2Nz4fFUELICsRrR/Nx4qdkk6cdnqJpI2Nxxsl3dhsHIIWQFbGMnVgu8/2i8OOvhZeYkpEDNRfKwYkXdbsG7gYBiArtVrrF8Miol9Sf7pq6ghaAFnpwPKuo7anRsSA7amSjjX7BqYOAGSlzRfDRrJN0rLG42WSftbsG+hoAWSlnW/Btb1Z0gJJk20flrRG0jpJW20vl3RQ0hebjUPQAshKOzeViYjbzvLUorGMQ9ACyAqbygBAYlV8Cy5BCyArdLQAkBgbfwNAYnS056kHf3CDFl9/hY4fH9Q189eXXQ5KcvRN6f77evX2W5Z7pJtuKbT0jkJ/2CetW9urDwatqdNCa9fVNGlS2dV2ryoGLW9Y6IBNj+zWzUu2ll0GStbbK9399Zq2bhvShk1DemxLj/b/Ufrmml6tWFlo8+NDWrCo0KMP8Ws5Hh14w8KY8S/aAc89e0jvnPhT2WWgZJM/Il05t/544kRp5szQ8aPWwQPWVfPrbdgnPxXa8TS/luNRi9aPTuFfFCjBkTekfa9YH/t4aNbs0M4d9e7q6Sd7dPTNkovrciG3fHTKOQet7X8e5bm/bD12cuj5c30JIEuDg9Lqeyboa6vqc7HfWFvTj7f06M5bJ2hwUJpwQdkVdrciWj86ZTwXw/5V0kMjPTF867G/unBdBaemgXIMnZJW3dOr6z5XaOFn678aM2ZJ3+uvSZJePyA9u7N6y5O6SRUDZ9Sgtf3S2Z6SNKX95QD5ipAeWNOrmbNCty8r/nL+xNvSJZdKRSFt6O/VzbcWo4yCZjrZqbaqWUc7RdJ1kt457bwlPZekogxt2PgFffozl+vSyRdq72t36VsPPKNHNp7tbxhy9fvfWb/8eY9mzwndfkt91u6ur9Z06KD12Jb65wsXFfr8jRVMii7SyYtcrWoWtE9ImhQRu05/wvZ/J6koQ19etq3sElAB8z4Ren73qRGeCS29gy62XSqYs6MHbUQsH+W5f2p/OQAwPlX8k8U7wwBkpes6WgDoNnS0AJBYFfc6IGgBZKVWdgEjIGgBZIWpAwBIjKAFgMQqOEVL0ALICx0tACQWFexpCVoAWWHVAQAkxtQBACQWZuoAAJKiowWAxAhaAEisxqoDAEirncu7bB+Q9L7qixmGImL+uYxD0ALISoKpg4UR8dZ4BiBoAWQlKngT4Z6yCwCAdioULR+2+2y/OOzoO224kPRftn8zwnMto6MFkJWxTB1ERL+k/lG+5NqIOGL7MklP2X4lInaOtSY6WgBZqSlaPpqJiCONj8ckPS7p6nOpiaAFkJWxTB2MxvZE2x/+82NJ/yhpz7nUxNQBgKy08WLYFEmP25bqWfmjiPjVuQxE0ALISrNOtVURsV/S37djLIIWQFbYjxYAEmOvAwBIjL0OACCxgv1oASCtdl0MayeCFkBWqhezBC2AzNDRAkBiQwQtAKTFOloASIypAwBIjOVdAJAY7wwDgMSYOgCAxGoV7GkJWgBZoaMFgMQIWgBIjKAFgMSK9t3Kpm0IWgBZoaMFgMROseoAANKiowWAxAhaAEisZqYOACApbs4IAImdrGBH64jqpX+ubPdFRH/ZdaBa+LnIX0/ZBZxn+souAJXEz0XmCFoASIygBYDECNrOYh4OI+HnInNcDAOAxOhoASAxghYAEiNoO8T2Ytv7bL9me3XZ9aB8tjfYPmZ7T9m1IC2CtgNs90p6UNL1kuZKus323HKrQgU8LGlx2UUgPYK2M66W9FpE7I+Ik5K2SFpSck0oWUTslHSi7DqQHkHbGdMkHRr2+eHGOQDnAYK2M0a6ixHr6oDzBEHbGYclTR/2+d9IOlJSLQA6jKDtjBckzbE90/aHJC2VtK3kmgB0CEHbARExJGmFpCcl7ZW0NSJeLrcqlM32Zkn/I+nvbB+2vbzsmpAGb8EFgMToaAEgMYIWABIjaAEgMYIWABIjaAEgMYIWABIjaAEgsf8DX3QkvgO1dswAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm,cmap=\"plasma\",annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
